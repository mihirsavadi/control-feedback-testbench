@article{hagannueralcontrols,
    author = {Hagan, Martin T. and Demuth, Howard B. and Jesús, Orlando De},
    title = {An introduction to the use of neural networks in control systems},
    journal = {International Journal of Robust and Nonlinear Control},
    volume = {12},
    number = {11},
    pages = {959-985},
    keywords = {neurocontrol, model reference control, model predictive control, feedback linearization},
    doi = {https://doi.org/10.1002/rnc.727},
    url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rnc.727},
    eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/rnc.727},
    abstract = {Abstract The purpose of this paper is to provide a quick overview of neural networks and to explain how they can be used in control systems. We introduce the multilayer perceptron neural network and describe how it can be used for function approximation. The backpropagation algorithm (including its variations) is the principal procedure for training multilayer perceptrons; it is briefly described here. Care must be taken, when training perceptron networks, to ensure that they do not overfit the training data and then fail to generalize well in new situations. Several techniques for improving generalization are discussed. The paper also presents three control architectures: model reference adaptive control, model predictive control, and feedback linearization control. These controllers demonstrate the variety of ways in which multilayer perceptron neural networks can be used as basic building blocks. We demonstrate the practical implementation of these controllers on three applications: a continuous stirred tank reactor, a robot arm, and a magnetic levitation system. Copyright © 2002 John Wiley \& Sons, Ltd.},
    year = {2002}
}

@article{BARTO1994888,
    title = {Reinforcement learning control},
    journal = {Current Opinion in Neurobiology},
    volume = {4},
    number = {6},
    pages = {888-893},
    year = {1994},
    issn = {0959-4388},
    doi = {https://doi.org/10.1016/0959-4388(94)90138-4},
    url = {https://www.sciencedirect.com/science/article/pii/0959438894901384},
    author = {Andrew G. Barto},
    abstract = {Reinforcement learning refers to improving performance through trial-and-error. Despite recent progress in developing artificial learning systems, including new learning methods for artificial neural networks, most of these systems learn under the tutelage of a knowledgeable ‘teacher’ able to tell them how to respond to a set of training stimuli. Learning under these conditions is not adequate, however, when it is costly, or even impossible, to obtain this kind of training information. Reinforcement learning is attracting increasing attention in computer science and engineering because it can be used by autonomous systems to learn from their experiences instead of from knowledgeable teachers, and it is attracting attention in computational neuroscience because it is consonant with biological principles. Recent research has improved the efficiency of reinforcement learning and has provided some striking examples of its capabilities.}
}

@article{PONKUMAR2018512,
    title = {A Deep Learning Architecture for Predictive Control},
    journal = {IFAC-PapersOnLine},
    volume = {51},
    number = {18},
    pages = {512-517},
    year = {2018},
    note = {10th IFAC Symposium on Advanced Control of Chemical Processes ADCHEM 2018},
    issn = {2405-8963},
    doi = {https://doi.org/10.1016/j.ifacol.2018.09.373},
    url = {https://www.sciencedirect.com/science/article/pii/S2405896318320597},
    author = {Steven Spielberg {Pon Kumar} and Aditya Tulsyan and Bhushan Gopaluni and Philip Loewen},
    keywords = {artificial intelligence, model predictive control, deep neural networks, optimization},
    abstract = {Model predictive control (MPC) is a popular control strategy that computes control actions by solving an optimization problem in real-time. Uncertainty and nonlinearity of a process, and the non-convexity of the resulting optimization problem can make online implementation of MPC nontrivial. Consequently, MPC is most often used in processes where the time constants are large and/or high-performance computing support is available. We propose a deep neural network (DNN) controller architecture to reduce the computational cost of implementing an MPC. This is done by training a DNN controller on simulated input-output data from a well-designed MPC. The online implementation of a DNN controller does not require solving an optimization problem. Once the DNN is trained, the MPC is fully replaced with the DNN controller. The benefits of this approach are illustrated through a simulated example.}
}

@inbook{14354d8827c14edd9a2cbf70cf08b465,
    title = "An adaptive critic global controller",
    abstract = "A nonlinear control system comprising a network of networks is taught using a two-phase learning procedure realized through novel techniques for initialization, on-line training, and adaptive critic design. The neural networks are initialized algebraically by observing that the gradients of the networks must equal corresponding linear gain matrices at chosen operating points. On-line learning is based on a dual heuristic adaptive critic architecture that improves control for large, coupled motions by accounting for plant dynamics and nonlinear effects. The result is an adaptive controller that is as conservative as the linear designs and as effective as the global controller. The design method is implemented to control the full six-degree-of-freedom simulation of a business jet aircraft.",
    author = "Silvia Ferrari and Stengel, {Robert F.}",
    year = "2002",
    doi = "10.1109/ACC.2002.1025189",
    language = "English (US)",
    isbn = "0780372980",
    volume = "4",
    pages = "2665--2670",
    booktitle = "Proceedings of the American Control Conference",
}

@ARTICLE{55119,
    author={Nguyen, D.H. and Widrow, B.},
    journal={IEEE Control Systems Magazine}, 
    title={Neural networks for self-learning control systems}, 
    year={1990},
    volume={10},
    number={3},
    pages={18-23},
    doi={10.1109/37.55119}}

@INBOOK{6300635,
    author={Miller, W. Thomas and Sutton, Richard S. and Werbos, Paul J.},
    booktitle={Neural Networks for Control}, 
    title={Computational Schemes and Neural Network Models for Formation and Control of Multijoint Arm Trajectory}, 
    year={1995},
    volume={},
    number={},
    pages={197-228},
    doi={}}

@article{article,
    author = {Zhao, Jun and Zeng, Qingliang and Guo, Bin},
    year = {2021},
    month = {11},
    pages = {1-8},
    title = {Adaptive Critic Learning-Based Robust Control of Systems with Uncertain Dynamics},
    volume = {2021},
    journal = {Computational Intelligence and Neuroscience},
    doi = {10.1155/2021/2952115}
    }

@inproceedings{Liu2006AnIT,
    title={An Introduction to Adaptive Critic Control: A Paradigm Based on Approximate Dynamic Programming},
    author={Derong Liu},
    year={2006}
}

@article{Nicholas,
    author   = "Nicholas Lewis",
    title    = "Emulating a PID Controller with Long Short-term Memory, Series",
    journal  = "towardsdatascience.com",
    year     = 2020,
    url      = "\url{https://towardsdatascience.com/emulating-a-pid-controller-with-long-short-term-memory-part-1-bb5b87165b08}",
}

@article{Andrej,
    author   = "Andrej Karpathy",
    title    = "The Unreasonable Effectiveness of Recurrent Neural Networks",
    journal  = "\url{https://karpathy.github.io/2015/05/21/rnn-effectiveness/}",
    year     = 2015,
    }

@article{Colah,
    author   = "Christopher Olah",
    title    = "Understanding LSTM Networks",
    journal  = "\url{https://colah.github.io/posts/2015-08-Understanding-LSTMs/}",
    year     = 2015,
}

@book{lewis2020neural,
  title={Neural Network Control Of Robot Manipulators And Non-Linear Systems},
  author={Lewis, F.W. and Jagannathan, S. and Yesildirak, A.},
  isbn={9781000162776},
  url={https://books.google.com/books?id=1D31DwAAQBAJ},
  year={2020},
  publisher={CRC Press}
}
