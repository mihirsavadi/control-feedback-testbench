@article{hagannueralcontrols,
    author = {Hagan, Martin T. and Demuth, Howard B. and Jesús, Orlando De},
    title = {An introduction to the use of neural networks in control systems},
    journal = {International Journal of Robust and Nonlinear Control},
    volume = {12},
    number = {11},
    pages = {959-985},
    keywords = {neurocontrol, model reference control, model predictive control, feedback linearization},
    doi = {\url{https://doi.org/10.1002/rnc.727}},
    note = {\url{https://onlinelibrary.wiley.com/doi/abs/10.1002/rnc.727}},
    eprint = {\url{https://onlinelibrary.wiley.com/doi/pdf/10.1002/rnc.727}},
    abstract = {Abstract The purpose of this paper is to provide a quick overview of neural networks and to explain how they can be used in control systems. We introduce the multilayer perceptron neural network and describe how it can be used for function approximation. The backpropagation algorithm (including its variations) is the principal procedure for training multilayer perceptrons; it is briefly described here. Care must be taken, when training perceptron networks, to ensure that they do not overfit the training data and then fail to generalize well in new situations. Several techniques for improving generalization are discussed. The paper also presents three control architectures: model reference adaptive control, model predictive control, and feedback linearization control. These controllers demonstrate the variety of ways in which multilayer perceptron neural networks can be used as basic building blocks. We demonstrate the practical implementation of these controllers on three applications: a continuous stirred tank reactor, a robot arm, and a magnetic levitation system. Copyright © 2002 John Wiley \& Sons, Ltd.},
    year = {2002}
}

@INPROCEEDINGS{786109,
    author={Hagan, M.T. and Demuth, H.B.},
    booktitle={Proceedings of the 1999 American Control Conference (Cat. No. 99CH36251)}, 
    title={Neural networks for control}, 
    year={1999},
    volume={3},
    number={},
    pages={1642-1656 vol.3},
    doi={10.1109/ACC.1999.786109}
}

@article{BARTO1994888,
    title = {Reinforcement learning control},
    journal = {Current Opinion in Neurobiology},
    volume = {4},
    number = {6},
    pages = {888-893},
    year = {1994},
    issn = {0959-4388},
    doi = {\url{https://doi.org/10.1016/0959-4388(94)90138-4}},
    note = {\url{https://www.sciencedirect.com/science/article/pii/0959438894901384}},
    author = {Andrew G. Barto},
    abstract = {Reinforcement learning refers to improving performance through trial-and-error. Despite recent progress in developing artificial learning systems, including new learning methods for artificial neural networks, most of these systems learn under the tutelage of a knowledgeable 'teacher' able to tell them how to respond to a set of training stimuli. Learning under these conditions is not adequate, however, when it is costly, or even impossible, to obtain this kind of training information. Reinforcement learning is attracting increasing attention in computer science and engineering because it can be used by autonomous systems to learn from their experiences instead of from knowledgeable teachers, and it is attracting attention in computational neuroscience because it is consonant with biological principles. Recent research has improved the efficiency of reinforcement learning and has provided some striking examples of its capabilities.}
}

@article{PONKUMAR2018512,
    title = {A Deep Learning Architecture for Predictive Control},
    journal = {IFAC-PapersOnLine},
    volume = {51},
    number = {18},
    pages = {512-517},
    year = {2018},
    note = {10th IFAC Symposium on Advanced Control of Chemical Processes ADCHEM 2018},
    issn = {2405-8963},
    doi = {\url{https://doi.org/10.1016/j.ifacol.2018.09.373}},
    note = {\url{https://www.sciencedirect.com/science/article/pii/S2405896318320597}},
    author = {Steven Spielberg {Pon Kumar} and Aditya Tulsyan and Bhushan Gopaluni and Philip Loewen},
    keywords = {artificial intelligence, model predictive control, deep neural networks, optimization},
    abstract = {Model predictive control (MPC) is a popular control strategy that computes control actions by solving an optimization problem in real-time. Uncertainty and nonlinearity of a process, and the non-convexity of the resulting optimization problem can make online implementation of MPC nontrivial. Consequently, MPC is most often used in processes where the time constants are large and/or high-performance computing support is available. We propose a deep neural network (DNN) controller architecture to reduce the computational cost of implementing an MPC. This is done by training a DNN controller on simulated input-output data from a well-designed MPC. The online implementation of a DNN controller does not require solving an optimization problem. Once the DNN is trained, the MPC is fully replaced with the DNN controller. The benefits of this approach are illustrated through a simulated example.}
}

@inbook{14354d8827c14edd9a2cbf70cf08b465,
    title = "An adaptive critic global controller",
    abstract = "A nonlinear control system comprising a network of networks is taught using a two-phase learning procedure realized through novel techniques for initialization, on-line training, and adaptive critic design. The neural networks are initialized algebraically by observing that the gradients of the networks must equal corresponding linear gain matrices at chosen operating points. On-line learning is based on a dual heuristic adaptive critic architecture that improves control for large, coupled motions by accounting for plant dynamics and nonlinear effects. The result is an adaptive controller that is as conservative as the linear designs and as effective as the global controller. The design method is implemented to control the full six-degree-of-freedom simulation of a business jet aircraft.",
    author = "Silvia Ferrari and Stengel, {Robert F.}",
    year = "2002",
    doi = "10.1109/ACC.2002.1025189",
    language = "English (US)",
    isbn = "0780372980",
    volume = "4",
    pages = "2665--2670",
    booktitle = "Proceedings of the American Control Conference",
}

@ARTICLE{55119,
    author={Nguyen, D.H. and Widrow, B.},
    journal={IEEE Control Systems Magazine}, 
    title={Neural networks for self-learning control systems}, 
    year={1990},
    volume={10},
    number={3},
    pages={18-23},
    doi={10.1109/37.55119}}

@INBOOK{6300635,
    author={Miller, W. Thomas and Sutton, Richard S. and Werbos, Paul J.},
    booktitle={Neural Networks for Control}, 
    title={Computational Schemes and Neural Network Models for Formation and Control of Multijoint Arm Trajectory}, 
    year={1995},
    volume={},
    number={},
    pages={197-228},
    doi={}
}

@article{article,
    author = {Zhao, Jun and Zeng, Qingliang and Guo, Bin},
    year = {2021},
    month = {11},
    pages = {1-8},
    title = {Adaptive Critic Learning-Based Robust Control of Systems with Uncertain Dynamics},
    volume = {2021},
    journal = {Computational Intelligence and Neuroscience},
    doi = {10.1155/2021/2952115}
}

@inproceedings{Liu2006AnIT,
    title={An Introduction to Adaptive Critic Control: A Paradigm Based on Approximate Dynamic Programming},
    author={Derong Liu},
    year={2006}
}

@article{Nicholas,
    author   = "Nicholas Lewis",
    title    = "Emulating a PID Controller with Long Short-term Memory, Series",
    journal  = "towardsdatascience.com",
    year     = 2020,
    url      = "\url{https://towardsdatascience.com/emulating-a-pid-controller-with-long-short-term-memory-part-1-bb5b87165b08}",
}

@article{Andrej,
    author   = "Andrej Karpathy",
    title    = "The Unreasonable Effectiveness of Recurrent Neural Networks",
    journal  = "\url{https://karpathy.github.io/2015/05/21/rnn-effectiveness/}",
    year     = 2015,
}

@article{Colah,
    author   = "Christopher Olah",
    title    = "Understanding LSTM Networks",
    journal  = "\url{https://colah.github.io/posts/2015-08-Understanding-LSTMs/}",
    year     = 2015,
}

@book{lewis2020neural,
    title={Neural Network Control Of Robot Manipulators And Non-Linear Systems},
    author={Lewis, F.W. and Jagannathan, S. and Yesildirak, A.},
    isbn={9781000162776},
    url={\url{https://books.google.com/books?id=1D31DwAAQBAJ}},
    year={2020},
    publisher={CRC Press}
}

@article{lakhani,
    doi = {10.48550/ARXIV.2112.15187},
    note = {\url{https://arxiv.org/abs/2112.15187}},
    author = {Lakhani, Ayub I. and Chowdhury, Myisha A. and Lu, Qiugang},
    keywords = {Systems and Control (eess.SY), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
    title = {Stability-Preserving Automatic Tuning of PID Control with Reinforcement Learning},
    publisher = {arXiv},
    year = {2021},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{lakhani-2,
    note = {\url{https://www.diva-portal.org/smash/record.jsf?pid=diva2%3A1477561&dswid=-1462}},
    author = {Winqvist, Rebecka},
    title = {Neural Network Approaches for Model Predictive Control},
    publisher = {KTH ROYAL INSTITUTE OF TECHNOLOGY},
    year = {2020},
    abstract = {Model Predictive Control (MPC) is an optimization-based paradigm forfeedback control. The MPC relies on a dynamical model to make predictionsfor the future values of the controlled variables of the system. It then solvesa constrained optimization problem to calculate the optimal control actionthat minimizes the difference between the predicted values and the desiredor set values. One of the main limitations of the traditional MPC lies in thehigh computational cost resulting from solving the associated optimizationproblem online. Various offline strategies have been proposed to overcomethis, ranging from the explicit MPC (eMPC) to the recent learning-basedneural network approaches. This thesis investigates a framework for thetraining and evaluation of a neural network for learning to implement theMPC. As a part of the framework, a new approach for efficient generationof training data is proposed. Four different neural network structures arestudied; one of them is a black box network while the other three employMPC specific information. The networks are evaluated in terms of twodifferent performance metrics through experiments conducted on realistictwo-dimensional and four-dimensional systems. The experiments revealthat while using MPC specific structure in the neural networks resultsin performance gains when the training data is limited, all the networkstructures perform similarly as extensive training data is used. They furthershow that a recurrent neural network structure trained on both the state andcontrol trajectories of a family of MPCs is able to generalize to previouslyunseen MPC problems. The proposed methods in this thesis act as a firststep towards developing a coherent framework for characterization of learningapproaches in terms of both model validation and efficient training datageneration.}
}

@inproceedings{biswas_hvacpaper,
    author = {Biswas, Debmalya},
    title = {Reinforcement Learning Based HVAC Optimization in Factories},
    year = {2020},
    isbn = {9781450380096},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    note = {\url{https://doi.org/10.1145/3396851.3402363}},
    doi = {10.1145/3396851.3402363},
    abstract = {Heating, Ventilation and Air Conditioning (HVAC) units are responsible for maintaining the temperature and humidity settings in a building. Studies have shown that HVAC accounts for almost 50% energy consumption in a building and 10% of global electricity usage. HVAC optimization thus has the potential to contribute significantly towards our sustainability goals, reducing energy consumption and CO2 emissions. In this work, we explore ways to optimize the HVAC controls in factories. Unfortunately, this is a complex problem as it requires computing an optimal state considering multiple variable factors, e.g. the occupancy, manufacturing schedule, temperature requirements of operating machines, air flow dynamics within the building, external weather conditions, energy savings, etc. We present a Reinforcement Learning (RL) based energy optimization model that has been applied in our factories. We show that RL is a good fit as it is able to learn and adapt to multi-parameterized system dynamics in real-time. It provides around 25% energy savings on top of the previously used Proportional-Integral-Derivative (PID) controllers.},
    booktitle = {Proceedings of the Eleventh ACM International Conference on Future Energy Systems},
    pages = {428-433},
    numpages = {6},
    keywords = {Reinforcement Learning, Machine Learning, Sustainability, HVAC, Energy Optimization},
    location = {Virtual Event, Australia},
    series = {e-Energy '20}
}

@misc{biswas_2022, 
    title = {Data-Driven (Reinforcement Learning-Based) Control}, 
    note = {\url{https://towardsdatascience.com/data-driven-control-d516ca28047c}}, 
    journal = {\url{https://towardsdatascience.com/}}, 
    author = {Biswas, Debmalya}, 
    year = {2022}, 
    month = {Feb},
    abstract = {Why Reinforcement Learning is such a hot topic for control-systems optimization problems in general.}
}

@misc{ramadhan_2021, 
    title = {Radial Basis Function Neural Network Simplified}, 
    note = {\url{https://towardsdatascience.com/radial-basis-function-neural-network-simplified-6f26e3d5e04d}}, 
    journal = {\url{https://towardsdatascience.com/}}, 
    author = {Ramadhan, Luthfi}, 
    year = {2021}, 
    month = {Nov},
    abstract = {A short introduction to radial basis function neural network.}
}

@INPROCEEDINGS{9207641,
    author={Guan, Zhe and Yamamoto, Toru},
    booktitle={2020 International Joint Conference on Neural Networks (IJCNN)}, 
    title={Design of a Reinforcement Learning PID controller}, 
    year={2020},
    volume={},
    number={},
    pages={1-6},
    doi={10.1109/IJCNN48605.2020.9207641}
}

@MISC{wikipediaPIDdiagram,
    author = {Arturo Urquizo},
    title = {PID.svg},
    year = {2008},
    note = {\url{https://commons.wikimedia.org/wiki/File:PID.svg}}
}

@software{gitrepo,
  author = {Mihir Savadi},
  month = {May},
  title = {{Control Feedback Testbench Repository}},
  url = {\url{https://github.com/mihirsavadi/control-feedback-testbench}},
  note = {\url{https://github.com/mihirsavadi/control-feedback-testbench}},
  version = {0.0.1},
  year = {2022}
}